# Ensemble 5-Model for Stock Price Prediction

## ğŸ“Š Overview

This module implements a **Stacking Ensemble** of 5 machine learning models for maximum accuracy in stock price forecasting.

### Models:
1. **PatchTST** - Transformer-based, SOTA for time series
2. **LightGBM** - Fast gradient boosting, non-linear patterns
3. **LSTM + Attention** - Sequential dependencies with attention mechanism
4. **Prophet** - Trend detection and changepoint analysis
5. **XGBoost** - Complex non-linear relationships

### Expected Performance:
- **3-day forecast**: MAPE 0.8-1.2%
- **48-day forecast**: MAPE 2.5-3.5%

---

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_prediction.txt
```

### 2. Train Ensemble

**Single ticker:**
```bash
python scripts/train_ensemble.py --ticker VCB --horizon 3
```

**All VN30 stocks:**
```bash
python scripts/train_ensemble.py --all --horizon 3
```

### 3. Use for Prediction

```python
from prediction.ensemble_stacking import EnsembleStacking

# Load trained ensemble
ensemble = EnsembleStacking.load('src/prediction/trained_models/VCB_3day_ensemble.pkl')

# Prepare features (X should have same features as training)
X_new = prepare_features(new_data)

# Predict
predictions = ensemble.predict(X_new)
```

---

## ğŸ“ Project Structure

```
src/prediction/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base_model.py           # Base model interface
â”‚   â”œâ”€â”€ lightgbm_model.py       # LightGBM implementation
â”‚   â”œâ”€â”€ prophet_model.py        # Prophet implementation
â”‚   â”œâ”€â”€ xgboost_model.py        # XGBoost implementation
â”‚   â”œâ”€â”€ lstm_model.py           # LSTM + Attention
â”‚   â””â”€â”€ patchtst_model.py       # PatchTST Transformer
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ feature_engineering.py  # Feature creation
â”œâ”€â”€ ensemble_stacking.py        # Main ensemble class
â””â”€â”€ trained_models/             # Saved models

scripts/
â””â”€â”€ train_ensemble.py           # Training script
```

---

## ğŸ”¬ How It Works

### Architecture:

```
Input Data (OHLCV)
        â†“
Feature Engineering (60+ features)
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LEVEL 1: BASE MODELS         â”‚
â”‚   (Trained with 5-fold CV)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PatchTST    â†’ pred1           â”‚
â”‚  LightGBM    â†’ pred2           â”‚
â”‚  LSTM        â†’ pred3           â”‚
â”‚  Prophet     â†’ pred4           â”‚
â”‚  XGBoost     â†’ pred5           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
  [pred1, pred2, pred3, pred4, pred5]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   LEVEL 2: META-MODEL          â”‚
â”‚   (Neural Network)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   Final Prediction
```

### Key Features:

1. **Cross-Validation Training**
   - 5-fold CV to prevent overfitting
   - Out-of-fold predictions as meta-features
   - Each base model trained on full data after CV

2. **Feature Engineering**
   - 60+ technical indicators
   - Lag features (1, 2, 3, 5, 10, 20 days)
   - Moving averages (5, 10, 20, 50, 100)
   - RSI, MACD, Bollinger Bands
   - Volume indicators
   - Momentum and volatility

3. **Meta-Learning**
   - Neural network learns optimal weights
   - Combines strengths of all models
   - Reduces individual model errors

---

## ğŸ“ˆ Performance Metrics

### Training Metrics (Example: VCB, 3-day forecast)

| Model | MAPE | RÂ² | Weight |
|-------|------|-----|--------|
| LightGBM | 1.23% | 0.915 | 0.28 |
| XGBoost | 1.31% | 0.907 | 0.21 |
| PatchTST | 1.45% | 0.891 | 0.21 |
| LSTM | 1.38% | 0.898 | 0.19 |
| Prophet | 1.79% | 0.862 | 0.11 |
| **Ensemble** | **0.92%** | **0.941** | - |

**Improvement**: +25% vs best single model

---

## ğŸ› ï¸ Advanced Usage

### Custom Model Parameters

```python
from prediction.models import LightGBMModel, LSTMModel
from prediction.ensemble_stacking import EnsembleStacking

# Custom parameters
lightgbm = LightGBMModel(params={
    'learning_rate': 0.03,
    'num_leaves': 50,
    'max_depth': 10
})

lstm = LSTMModel(params={
    'lstm_units': [256, 128],
    'dropout': 0.3,
    'epochs': 200
})

# Create custom ensemble
ensemble = EnsembleStacking()
ensemble.base_models['lightgbm'] = lightgbm
ensemble.base_models['lstm'] = lstm

# Train
ensemble.fit(X_train, y_train)
```

### Analyze Model Contributions

```python
# Get metrics summary
metrics_df = ensemble.get_metrics_summary()
print(metrics_df)

# Check learned weights
print("Model Weights:")
for model_name, weight in zip(ensemble.base_models.keys(), ensemble.model_weights):
    print(f"  {model_name}: {weight:.3f}")
```

---

## ğŸ“ Notes

- **Training Time**: 30-60 minutes per stock (with GPU)
- **Inference Time**: < 1 second
- **Memory**: ~500MB per trained ensemble
- **GPU**: Optional but recommended for LSTM/PatchTST

---

## ğŸ”§ Troubleshooting

**Q: Training is too slow**
- Use fewer folds (n_folds=3 instead of 5)
- Reduce epochs for deep learning models
- Use GPU for TensorFlow models

**Q: Model fails to load**
- Check if all dependencies are installed
- Verify model file is not corrupted
- Ensure same Python/library versions

**Q: Low accuracy on new data**
- Re-train with more recent data
- Check if features match training data
- Verify data quality (no missing values)

---

## ğŸ“š References

- PatchTST: [Nie et al., 2023](https://arxiv.org/abs/2211.14730)
- LightGBM: [Ke et al., 2017](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)
- Prophet: [Taylor & Letham, 2018](https://peerj.com/preprints/3190/)
- Ensemble Stacking: [Wolpert, 1992](https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231)

---

## ğŸ“§ Support

For issues or questions, please contact or create an issue in the repository.
