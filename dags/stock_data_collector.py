"""
Airflow DAG - Thu tháº­p dá»¯ liá»‡u cá»• phiáº¿u
Scheduler: Cháº¡y má»—i ngÃ y vÃ o lÃºc 15:30 chiá»u (sau khi thá»‹ trÆ°á»ng Ä‘Ã³ng cá»­a)
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import sys
import os

# Add src to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), ".."))

from src.data_collector.vnstock_client import VnStockClient
from src.kafka_producer.producer import StockDataProducer

# ================================================================================
# DAG Configuration
# ================================================================================

default_args = {
    "owner": "stock",
    "depends_on_past": False,
    "start_date": datetime(2025, 1, 1),
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "stock_data_collector",
    default_args=default_args,
    description="Crawl daily stock data - End of Day (sau khi thá»‹ trÆ°á»ng Ä‘Ã³ng cá»­a)",
    schedule_interval="30 15 * * 1-5",  # 15:30 chiá»u (sau khi Ä‘Ã³ng cá»­a)
    catchup=False,
    tags=["stock", "daily", "eod"],
)

# ============================================================================
# DANH SÃCH 50 Cá»” PHIáº¾U (VN30 + VNMidcap phá»• biáº¿n)
# ============================================================================

# Batch 1: Top 10 NgÃ¢n hÃ ng & TÃ i chÃ­nh
TICKERS_BATCH_1 = ["VCB", "BID", "CTG", "VPB", "TCB", "MBB", "ACB", "STB", "HDB", "SSI"]

# Batch 2: Top 10 Báº¥t Ä‘á»™ng sáº£n & XÃ¢y dá»±ng
TICKERS_BATCH_2 = ["VHM", "VIC", "VRE", "NVL", "PDR", "DXG", "KDH", "HDC", "DIG", "BCM"]

# Batch 3: Top 10 Thá»±c pháº©m & TiÃªu dÃ¹ng
TICKERS_BATCH_3 = ["VNM", "MSN", "MWG", "SAB", "VHC", "FRT", "MCH", "ASM", "DGW", "PNJ"]

# Batch 4: Top 10 CÃ´ng nghiá»‡p & NÄƒng lÆ°á»£ng
TICKERS_BATCH_4 = ["HPG", "GAS", "POW", "PLX", "PVD", "PVS", "PVT", "GEG", "NT2", "REE"]

# Batch 5: Top 10 CÃ´ng nghá»‡ & Dá»‹ch vá»¥
TICKERS_BATCH_5 = ["FPT", "VGC", "GMD", "SHB", "EVF", "VCI", "VIX", "HCM", "CMG", "ITD"]

# =============================================================================
# Task function
# =============================================================================


def crawl_and_produce_batch(tickers, batch_name):
    """
    Crawl data vÃ  gá»­i vÃ o kafka cho má»™t batch

    Args:
        tickers: List cÃ¡c mÃ£ cá»• phiáº¿u
        batch_name: TÃªn batch (Ä‘á»ƒ logging)
    """
    print(f"{'='*60}")
    print(f"Processing batch: {batch_name}")
    print(f"Tickers: {tickers}")
    print(f"{'='* 60}")

    client = VnStockClient()
    producer = StockDataProducer()

    success_count = 0
    failed_count = 0
    for ticker in tickers:
        try:
            print(f"ðŸ“Š Crawling {ticker}...")

            # Crawl data
            df = client.get_daily_data(ticker)

            if df is None or df.empty:
                print(f"âš ï¸ No data for {ticker}")
                failed_count += 1
                continue
            # Gá»­i vÃ o kafka
            success = producer.send_stock_data(ticker, df)

            if success:
                print(f"âœ… {ticker} sent successfully")
                success_count += 1
            else:
                print(f"âŒ Failed to send {ticker}")
                failed_count += 1

        except Exception as e:
            print(f"âŒ Error processing {ticker}: {e}")
            failed_count += 1

    # ÄÃ³ng producer
    producer.close()

    # Summary
    print(f"\n{'='*60}")
    print(f"Batch {batch_name} Summary:")
    print(f"    Total: {len(tickers)}")
    print(f"    âœ… Success: {success_count}")
    print(f"    âŒ Failed: {failed_count}")
    print(f"{'='*60}")

    # Raise error náº¿u táº¥t cáº£ Ä‘á»u fail
    if success_count == 0 and len(tickers) > 0:
        raise Exception(f"All tickers in batch {batch_name} failed!")

    return {"batch": batch_name, "success": success_count, "failed": failed_count}


# ===============================================================================
# TASKS - 5 batches cháº¡y song song
# ===============================================================================

task_batch_1 = PythonOperator(
    task_id="crawl_batch_1",
    python_callable=crawl_and_produce_batch,
    op_kwargs={"tickers": TICKERS_BATCH_1, "batch_name": "Batch 1 - Banks"},
    dag=dag,
)

task_batch_2 = PythonOperator(
    task_id="crawl_batch_2",
    python_callable=crawl_and_produce_batch,
    op_kwargs={"tickers": TICKERS_BATCH_2, "batch_name": "Batch 2 - Real Estate"},
    dag=dag,
)

task_batch_3 = PythonOperator(
    task_id="crawl_batch_3",
    python_callable=crawl_and_produce_batch,
    op_kwargs={"tickers": TICKERS_BATCH_3, "batch_name": "Batch 3 - Consumer"},
    dag=dag,
)

task_batch_4 = PythonOperator(
    task_id="crawl_batch_4",
    python_callable=crawl_and_produce_batch,
    op_kwargs={"tickers": TICKERS_BATCH_4, "batch_name": "Batch 4 - Industry"},
    dag=dag,
)

task_batch_5 = PythonOperator(
    task_id="crawl_batch_5",
    python_callable=crawl_and_produce_batch,
    op_kwargs={"tickers": TICKERS_BATCH_5, "batch_name": "Batch 5 - Technology"},
    dag=dag,
)

# All batches run in parallel (no dependencies)
# Airflow will execute them concurrently based on available workers

# ==========================================================================
# Task Dependencies (cháº¡y song song)
# ==========================================================================

[task_batch_1, task_batch_2, task_batch_3]

